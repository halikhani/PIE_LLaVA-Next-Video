{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    'hamidra/pie-llava-augmented',\n",
    "    token=\"\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextVideoProcessor\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"/mnt/esperanto/et/intern/hamidreza/PIE/hamidreza_files/checkpoints/run_aug_dataset\")\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(video_clip, model):\n",
    "\n",
    "    conversation = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"Above are 16 frames of a driving scenario captured by the ego vehicle camera based on the video taken, in which the pedestrian of interest is located with a red bounding box \"},\n",
    "            #   {\"type\": \"text\", \"text\": \"The normalized bounding boxes of the pedestrian in these consequetive 16 frames are provided as follows in 8 lists each containing 4 elements, with format of [x1, y1, x2, y2] in which x1 and y1 are coordinates of top left corner and x2 and y2 are coordinates of the bottom right corner of the bounding box: \"},\n",
    "            #   {\"type\": \"text\", \"text\": ped_bbox + \". \"},\n",
    "            #   {\"type\": \"text\", \"text\": ped_look + \". \"},\n",
    "            #   {\"type\": \"text\", \"text\": ped_action + \". \"},\n",
    "              {\"type\": \"text\", \"text\": \"Using these frames, provided context and pedestrian bounding box, and your reasoning, answer the following question only with ‘Yes’ or ‘No’. You may use your knowledge if needed. DO NOT EXPLAIN your reasoning, be confident.\\nQuestion: Does the indicated pedestrian intend to cross the intersection in future frames of this video?\"},\n",
    "              {\"type\": \"video\"},\n",
    "              ]\n",
    "      },\n",
    "]\n",
    "\n",
    "    # Set add_generation_prompt to add the \"ASSISTANT: \" at the end\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=None, # we have a processed video, passing it again to processor causes errors\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    video_clip = video_clip.to(model.device)\n",
    "\n",
    "    out = model.generate(**batch, pixel_values_videos=video_clip, max_length=MAX_LENGTH, do_sample=True, temperature=1)\n",
    "    generated_text = processor.batch_decode(out, skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_2(video_clip, context, model):\n",
    "\n",
    "    conversation = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": context},\n",
    "              {\"type\": \"text\", \"text\": \"Using these frames, provided context, and your reasoning, answer the following question only with ‘Yes’ or ‘No’. You may use your knowledge if needed. DO NOT EXPLAIN your reasoning, be confident.\\nQuestion: Does the indicated pedestrian intend to cross the intersection in future frames of this video?\"},\n",
    "              {\"type\": \"video\"},\n",
    "              ],\n",
    "      },\n",
    "]\n",
    "\n",
    "    # Set add_generation_prompt to add the \"ASSISTANT: \" at the end\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=None, # we have a processed video, passing it again to processor causes errors\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    video_clip = video_clip.to(model.device)\n",
    "\n",
    "    out = model.generate(**batch, pixel_values_videos=video_clip, max_length=MAX_LENGTH, do_sample=True)\n",
    "    generated_text = processor.batch_decode(out, skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/esperanto/et/intern/hamidreza/PIE\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "val_dataset_p = load_from_disk('hamidreza_files/hf_datasets_16frames/val_p/original')\n",
    "val_dataset_n = load_from_disk('hamidreza_files/hf_datasets_16frames/val_n/original')\n",
    "val_dataset = concatenate_datasets([val_dataset_p, val_dataset_n]).with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 3, 336, 336])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_example = val_dataset[0]\n",
    "inf_example[\"pixel_values_videos\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "print(processor.batch_decode(inf_example[\"input_ids\"])[0].split(' ')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 243/243 [01:22<00:00,  2.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def unsqueeze_pixel_values(example):\n",
    "    for k in example.keys():\n",
    "        if example[k].shape[0] == 1:\n",
    "            continue\n",
    "        else:\n",
    "            example[k] = example[k].unsqueeze(0)\n",
    "    return example\n",
    "\n",
    "val_dataset = val_dataset.map(unsqueeze_pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xp = load_from_disk('hamidreza_files/hf_datasets_16frames/test_p/original')\n",
    "# xn = load_from_disk('hamidreza_files/hf_datasets_16frames/test_n/original')\n",
    "# x = concatenate_datasets([xp, xn]).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "old_model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/LLaVa-NeXT-Video-7b-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# prev_model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "#     'hamidra/pie-llava-2',\n",
    "#     token=\"\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 243/243 [02:57<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model = model.to(\"cuda\")\n",
    "ft_model_ans_list = []\n",
    "\n",
    "for i in tqdm(range(len(val_dataset))):\n",
    "    gt = processor.batch_decode(val_dataset[i][\"input_ids\"])[0].split(' ')[-2]\n",
    "    pixel_vals = val_dataset[i][\"pixel_values_videos\"]\n",
    "    gen_ans = run_inference(pixel_vals, model)\n",
    "    bin_ans = gen_ans[0].strip('').split('ASSISTANT:')[1]\n",
    "    ft_model_ans_list.append([bin_ans, gt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_llavaNextVideo_perf(pred_list, ped_dir):\n",
    "\n",
    "    with open(ped_dir, 'r') as json_file:\n",
    "        ped_intention_action_dict = json.load(json_file)\n",
    "    print(len(pred_list))\n",
    "    print(len(ped_intention_action_dict))\n",
    "    count = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    idx = 0\n",
    "    for k, v in ped_intention_action_dict.items():\n",
    "        gt_intention_prob = ped_intention_action_dict[k][0]\n",
    "        gt_intention_action = ped_intention_action_dict[k][1]\n",
    "\n",
    "        # pred_prob = v[1]\n",
    "        pred_action = pred_list[idx].lower()\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "        # true positive\n",
    "        if 'yes' in pred_action and int(gt_intention_action) == 1:\n",
    "            tp += 1\n",
    "            count += 1\n",
    "        \n",
    "        # false positive\n",
    "        if 'yes' in pred_action and int(gt_intention_action) == 0:\n",
    "            fp += 1\n",
    "        \n",
    "        # true negative\n",
    "        if 'no' in pred_action and int(gt_intention_action) == 0:\n",
    "            tn += 1\n",
    "            count += 1\n",
    "\n",
    "        # false negative\n",
    "        if 'no' in pred_action and int(gt_intention_action) == 1:\n",
    "            fn += 1\n",
    "        \n",
    "        \n",
    "    # print(tp)\n",
    "    # print(fp)\n",
    "    # print(tn)\n",
    "    # print(fn)\n",
    "    # print(tp + fp + tn + fn)\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision + recall)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print('F1 score: ' + str(f1))\n",
    "    print(count/len(pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_llavaNextVideo_perf_2(pred_list):\n",
    "\n",
    "\n",
    "    print(len(pred_list))\n",
    "\n",
    "    count = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    idx = 0\n",
    "    for x in pred_list:\n",
    "        llm_answer = x[0].lower()\n",
    "        gt_answer = x[1].lower()\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "        # true positive\n",
    "        if 'yes' in llm_answer and 'yes' in gt_answer:\n",
    "            tp += 1\n",
    "            count += 1\n",
    "        \n",
    "        # false positive\n",
    "        if 'yes' in llm_answer and 'no' in gt_answer:\n",
    "            fp += 1\n",
    "        \n",
    "        # true negative\n",
    "        if 'no' in llm_answer and 'no' in gt_answer:\n",
    "            tn += 1\n",
    "            count += 1\n",
    "\n",
    "        # false negative\n",
    "        if 'no' in llm_answer and 'yes' in gt_answer:\n",
    "            fn += 1\n",
    "        \n",
    "        \n",
    "    # print(tp)\n",
    "    # print(fp)\n",
    "    # print(tn)\n",
    "    # print(fn)\n",
    "    # print(tp + fp + tn + fn)\n",
    "    print(tp)\n",
    "    print(fp)\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision + recall)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print('F1 score: ' + str(f1))\n",
    "    print(count/len(pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "155\n",
      "26\n",
      "0.856353591160221\n",
      "0.856353591160221\n",
      "F1 score: 0.856353591160221\n",
      "0.7860082304526749\n"
     ]
    }
   ],
   "source": [
    "# tst = ['yes' for i in range(len(ft_model_ans_list))]\n",
    "get_llavaNextVideo_perf_2(ft_model_ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base llava-next (no ft): acc: 64.6 (tmp: 1)\n",
    "# ft with original data: acc: 67.07 (tmp: 1)\n",
    "# ft with augmentation datset: acc:  75.9(tmp: 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-next",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
